**Stanford University 2014 (Wu Enda) Machine Learning Course Chinese Notes**

course address：<https://www.coursera.org/course/ml>

**Machine Learning** (Machine Learning) is the study of how computers simulate or implement human learning behaviors to acquire new knowledge or skills, and reorganize existing knowledge structures to continuously improve their own performance. It is the core of artificial intelligence and the fundamental way to make computers intelligent. Its application pervades all fields of artificial intelligence. It mainly uses induction and synthesis instead of deduction. Over the past decade, machine learning has helped us drive self-driving cars, perform efficient speech recognition, perform efficient web searches, and dramatically improve our understanding of the human genome. Machine learning is so prevalent today that you probably use it dozens of times a day without knowing it. Many researchers also believe that this is the best way to obtain artificial intelligence. In this class, you'll learn the most effective machine learning techniques and gain practice putting them to work for yourself. What's more, you'll learn not only the theoretical foundations, but also the practical techniques that require fast and powerful application of techniques to solve problems. Finally, you'll learn about some of Silicon Valley's best practice innovations utilizing machine learning and artificial intelligence.

This course provides a broad introduction to machine learning, data mining, and statistical pattern recognition. Topics include：

（一）Supervised learning (parametric/nonparametric algorithms, support vector machines, kernel functions, neural networks)。

（二）Unsupervised learning (clustering, dimensionality reduction, recommendation system, deep learning recommendation)。

（三）Best Practices in Machine Learning (Bias/Variance Theory; Innovation Processes in Machine Learning and AI). The course will also use extensive case studies and you will also learn how to use learning algorithms to build intelligent robots (perception, control), understanding of text (**Web** search, anti-spam), computer vision, medical informatics, audio , data mining, and other fields.

This course requires a total of 18 lessons in 10 weeks. Compared with the previous machine learning videos, this video is clearer, and each lesson has **ppt** courseware, which is recommended for learning.

I started to translate the subtitles of this course in the second half of 2014, and wrote the Chinese notes of the course. The notes have been downloaded tens of thousands of times, which should have helped many people, and many people have been helping me. Now I share the **word** and **markdown** manuscripts of the notes with you.

I will put **markdown** notes and course Chinese and English subtitles on **github**, I hope everyone can continue to improve. In order to facilitate the online display of mathematical formulas, the **html** file is viewed online, the formula has been converted into a picture, and the source code of the formula is in the **markdown** file。

**Finally, I want to say to my friends：**
**Gifts of roses, hand a fragrance！**
**On the road of artificial intelligence, you are not alone！**

Huang Haiguang

2018-3-26
[My Zhihu](https://www.zhihu.com/people/fengdu78/activities)

reference：https://www.coursera.org/course/ml Machine Learning Open Course

"Statistical Learning Methods" Li Hang
    
"Machine Learning Class" Zou Bo

## 备注：吴恩达老师的深度学习课（deepLearning.ai）的笔记地址：https://github.com/fengdu78/deeplearning_ai_books

-----------------------

Folder description:

**docx**: **word** version of notes

**markdown**: **markdown** version of notes

**html**: the **html** version of the note

**images**: pictures of notes

**ppt**: The original courseware of the course

**srt**: The Chinese and English subtitles of the course (**mp4** files need to be downloaded from Baidu Cloud, you can use Notepad or subtitle editing software to edit subtitles and improve them together, Baidu Cloud link: https://pan. baidu.com/s/1h8QjqBlOm0Exh7orm9teMQ Password: d3we, unzip after downloading)

**code**: **python** code of the course (some of them are written by foreign experts)

Machine learning video download link: https://pan.baidu.com/s/1raoOPOg Password: 48m8, including video and subtitles, unzip after downloading, it is recommended to use **potplayer** to play, this video is consistent with **mp4** .

[Notes online reading] (http://www.ai-start.com/ml2014)

Note pdf version download: see **github** root directory.

Machine learning qq group: 865189078 (we have 8 groups, if you add one, you don’t need to add it)

-----------------------

# Machine learning tutorial Chinese notes directory

- [Week 1](markdown/week1.md)

1. Introduction (**Introduction**)

1.1 welcome

1.2 What is machine learning?

1.3 Supervised Learning

1.4 Unsupervised Learning

2. Univariate linear regression (**Linear Regression with One Variable**)

2.1 Model Representation

2.2 Cost function

2.3 Intuitive understanding of cost function I

2.4 Intuitive Understanding of Cost Function II

2.5 Gradient Descent

2.6 Intuitive understanding of gradient descent

2.7 Linear Regression with Gradient Descent

2.8 What's Next

3. Linear Algebra Review (**Linear Algebra Review**)

3.1 Matrices and vectors

3.2 Addition and scalar multiplication

3.3 Matrix-vector multiplication

3.4 Matrix multiplication

3.5 Properties of matrix multiplication

3.6 Reverse, Transpose

- [Second week](markdown/week2.md)

4. Multivariate linear regression (**Linear Regression with Multiple Variables**)

4.1 Multidimensional features

4.2 Multivariate Gradient Descent

4.3 Gradient Descent Method Practice 1 - Feature Scaling

4.4 Gradient descent method practice 2-learning rate

4.5 Eigen and polynomial regression

4.6 Normal equations

4.7 Normal equations and irreversibility (optional)

5. Octave Tutorial (**Octave Tutorial**)

5.1 Basic operation

5.2 Mobile data

5.3 Calculation data

5.4 Drawing data

5.5 Control statements: **for**, **while**, **if** statements

5.6 Vectorization 88

5.7 Working and submitting programming exercises

- [Week 3](markdown/week3.md)

6. Logistic regression (**Logistic Regression**)

6.1 Classification problems

6.2 Hypothesis representation

6.3 Decision boundary

6.4 Cost Function

6.5 Simplified cost functions and gradient descent

6.6 Advanced optimization

6.7 Multi-class classification: one-vs-many

7. Regularization (**Regularization**)

7.1 The problem of overfitting

7.2 Cost Function

7.3 Regularized Linear Regression

7.4 Regularized Logistic Regression Models

- [Week 4](markdown/week4.md)

Eighth, neural network: representation (**Neural Networks: Representation**)

8.1 Nonlinear assumptions

8.2 Neurons and the Brain

8.3 Model Representation 1

8.4 Model Representation 2

8.5 Samples and intuitions1

8.6 Samples and Intuitive Understanding II

8.7 Multiclass classification

- [Week 5](markdown/week5.md)

Nine, neural network learning (**Neural Networks: Learning**)

9.1 Cost Function

9.2 Backpropagation Algorithm

9.3 Intuitive Understanding of the Backpropagation Algorithm

9.4 Implementation Note: Expand Parameters

9.5 Gradient testing

9.6 Random initialization

9.7 Putting it all together

9.8 Autonomous Driving

- [Week 6](markdown/week6.md)

10. Advice for Applying Machine Learning (**Advice for Applying Machine Learning**)

10.1 Deciding what to do next

10.2 Evaluating a hypothesis

10.3 Model selection and cross-validation sets

10.4 Diagnostic Bias and Variance

10.5 Regularization and bias/variance

10.6 Learning Curve

10.7 Deciding what to do next

11. Design of Machine Learning System (**Machine Learning System Design**)

11.1 What to do first

11.2 Error Analysis

11.3 Error Metrics for Class Skewness

11.4 The Trade-Off Between Precision and Recall

11.5 Data for Machine Learning

[Week 7](markdown/week7.md)

12. Support Vector Machines (**Support Vector Machines**)

12.1 Optimization objectives

12.2 Intuitive Understanding of Large Boundaries

12.3 Large-boundary classification behind the mathematics (optional)

12.4 Kernel function 1

12.5 Kernel functions 2

12.6 Using Support Vector Machines

- [Week 8](markdown/week8.md)

Thirteen, clustering (**Clustering**)

13.1 Unsupervised Learning: An Introduction

13.2 K-Means Algorithm

13.3 Optimization Objectives

13.4 Random initialization

13.5 Choosing the number of clusters

14. Dimensionality Reduction (**Dimensionality Reduction**)

14.1 Motivation One: Data Compression

14.2 Motivation Two: Data Visualization

14.3 Principal Component Analysis Problems

14.4 Principal Component Analysis Algorithm

14.5 Choosing the number of principal components

14.6 Compressed Representation for Reconstruction

14.7 Suggestions for the application of principal component analysis

- [Week 9](markdown/week9.md)

15. Anomaly Detection (**Anomaly Detection**)

15.1 Motivation for the question

15.2 Gaussian distribution

15.3 Algorithms

15.4 Developing and evaluating an anomaly detection system

15.5 Anomaly Detection vs. Supervised Learning

15.6 Selecting features

15.7 Multivariate Gaussian distribution (optional)

15.8 Anomaly Detection Using Multivariate Gaussian Distributions (Optional)

16. Recommender Systems (**Recommender Systems**)

16.1 Formalizing the problem

16.2 Content-Based Recommender Systems

16.3 Collaborative filtering

16.4 Collaborative filtering algorithm

16.5 Vectorization: Low-rank matrix factorization

16.6 Implementation Details: Mean Normalization

- [Week 10](markdown/week10.md)

Seventeen, large-scale machine learning (**Large Scale Machine Learning**)

17.1 Learning from Large Datasets

17.2 Stochastic Gradient Descent

17.3 Mini-batch gradient descent

17.4 Stochastic Gradient Descent Convergence

17.5 Online Learning

17.6 Map reduction and data parallelism

18. Application Example: Image Text Recognition (**Application Example: Photo OCR**)

20.1 Problem description and flowchart

18.2 Sliding windows

18.3 Acquiring large amounts of data and artificial data

18.4 Upper limit analysis: which part of the pipeline to do next

19. Summary (**Conclusion**)

19.1 Summary and Acknowledgments

------

